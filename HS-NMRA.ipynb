{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math,time,sys\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import seaborn as sns \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.naive_bayes import \n",
    "# from sklearn.ensemble import RanfomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MaxIter = 20\n",
    "pop_size = 20\n",
    "omega = 0.99\n",
    "bp = 0.5\n",
    "bsize = 4\n",
    "wsize = 16\n",
    "\n",
    "def initialise(partCount, dim, trainX, testX, trainy, testy):    \n",
    "    population=np.zeros((partCount,dim))\n",
    "    minn = 1\n",
    "    maxx = math.floor(0.5*dim)\n",
    "    \n",
    "    if maxx<minn:\n",
    "        maxx = minn + 1\n",
    "        #not(c[i].all())\n",
    "    \n",
    "    for i in range(partCount):\n",
    "        random.seed(i**3 + 10 + time.time() ) \n",
    "        no = random.randint(minn,maxx)\n",
    "        if no == 0:\n",
    "            no = 1\n",
    "        random.seed(time.time()+ 100)\n",
    "        pos = random.sample(range(0,dim-1),no)\n",
    "        for j in pos:\n",
    "            population[i][j]=1\n",
    "            \n",
    "    return population\n",
    "\n",
    "def fitness(agent, trainX, testX, trainy, testy):\n",
    "    # print(agent)\n",
    "    cols=np.flatnonzero(agent)\n",
    "    # print(cols)\n",
    "    val=1\n",
    "    if np.shape(cols)[0]==0:\n",
    "        return val  \n",
    "    clf = RandomForestClassifier(n_estimators=50)\n",
    "    #clf=KNeighborsClassifier(n_neighbors=50)\n",
    "    #clf=MLPClassifier(alpha=0.001, hidden_layer_sizes=(1000,500,100),max_iter=2000,random_state=4)\n",
    "    train_data=trainX[:,cols]\n",
    "    test_data=testX[:,cols]\n",
    "    clf.fit(train_data,trainy)\n",
    "    val=1-clf.score(test_data,testy)\n",
    "\n",
    "    #in case of multi objective  []\n",
    "    set_cnt=sum(agent)\n",
    "    set_cnt=set_cnt/np.shape(agent)[0]\n",
    "    val=omega*val+(1-omega)*set_cnt\n",
    "    return val\n",
    "\n",
    "def allfit(population, trainX, testX, trainy, testy):\n",
    "    x=np.shape(population)[0]\n",
    "    acc=np.zeros(x)\n",
    "    for i in range(x):\n",
    "        acc[i]=fitness(population[i],trainX, testX, trainy, testy)     \n",
    "        #print(acc[i])\n",
    "    return acc\n",
    "\n",
    "def test_accuracy(agent, trainX, testX, trainy, testy):\n",
    "    cols=np.flatnonzero(agent)\n",
    "    val=1\n",
    "    if np.shape(cols)[0]==0:\n",
    "        return val    \n",
    "    clf = RandomForestClassifier(n_estimators=50)\n",
    "    #clf=MLPClassifier(alpha=0.001, hidden_layer_sizes=(1000,500,100),max_iter=2000,random_state=4)\n",
    "    #clf=KNeighborsClassifier(n_neighbors=50)\n",
    "    # clf=MLPClassifier( alpha=0.01, max_iterno=1000) #hidden_layer_sizes=(1000,500,100)\n",
    "    #cross=4\n",
    "    #test_size=(1/cross)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(trainX, trainy,  stratify=trainy,test_size=test_size)\n",
    "    train_data=trainX[:,cols]\n",
    "    test_data=testX[:,cols]\n",
    "    clf.fit(train_data,trainy)\n",
    "    val=clf.score(test_data,testy)\n",
    "    return val\n",
    "\n",
    "def onecnt(agent):\n",
    "    return sum(agent)\n",
    "\n",
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1 - 1/(1 + math.exp(gamma))\n",
    "    else:\n",
    "        return 1/(1 + math.exp(-gamma))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HS(pop, fit, dimension, trainX, testX, trainy, testy):    \n",
    "    \n",
    "    hybrid = np.array([])\n",
    "    counter = 0\n",
    "\n",
    "    for j in range(dimension):\n",
    "        random.seed(j**3 + 10 + time.time())\n",
    "        ra = random.randint(0, pop_size-1)\n",
    "        hybrid = np.append(hybrid, pop[ra][j])\n",
    "\n",
    "    worst = pop[0]\n",
    "    for j in range(pop_size):\n",
    "        if(fit[j] > fitness(worst, trainX, testX, trainy, testy)):\n",
    "            worst = deepcopy(pop[j])\n",
    "            counter = j\n",
    "\n",
    "    if(fitness(worst, trainX, testX, trainy, testy) > fitness(hybrid, trainX, testX, trainy, testy)):\n",
    "        fit[counter] = deepcopy(fitness(hybrid, trainX, testX, trainy, testy))\n",
    "        pop[counter] = deepcopy(hybrid)\n",
    "\n",
    "    return pop, fit\n",
    "\n",
    "    \n",
    "def NMRA(dataset):\n",
    "    df = pd.read_csv(dataset)\n",
    "    a, b = np.shape(df)\n",
    "    data = df.values[:,0:b-1]\n",
    "    label = df.values[:,b-1]\n",
    "    dimension = data.shape[1]\n",
    "    \n",
    "    cross = 5\n",
    "    test_size = (1/cross)\n",
    "    trainX, testX, trainy, testy = train_test_split(data, label,stratify=label ,test_size=test_size,random_state=(7+17*int(time.time()%1000)))\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=50)\n",
    "    clf.fit(trainX,trainy)\n",
    "\n",
    "    val=clf.score(testX,testy)\n",
    "    whole_accuracy = val\n",
    "    print(\"Total Acc: \",val)\n",
    "    \n",
    "    pop = initialise(pop_size, dimension, trainX, testX, trainy, testy)\n",
    "    fit = allfit(pop, trainX, testX, trainy, testy)\n",
    "    \n",
    "    ind = np.argsort(fit)\n",
    "    index_b, index_w = [], []\n",
    "\n",
    "    for i in range(bsize):\n",
    "        index_b.append(ind[i])\n",
    "        \n",
    "    for i in range(wsize):\n",
    "        index_w.append(ind[i])\n",
    "    \n",
    "    bestpop = pop[ind[0]]\n",
    "    bestfit = fit[ind[0]]\n",
    "    \n",
    "    for i in range(MaxIter):\n",
    "        for j in range(pop_size):\n",
    "            pop, fit = HS(pop, fit, dimension, trainX, testX, trainy, testy)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for j in range(len(index_w)):\n",
    "            l = []\n",
    "            for k in range(dimension):\n",
    "                random.seed(time.time()+k*2)\n",
    "                l.append(random.random())\n",
    "                \n",
    "            pos = random.sample(range(0,len(index_w)-1),2)\n",
    "            s = np.add(pop[index_w[j]], np.multiply(l, np.subtract(pop[index_w[pos[0]]],pop[index_w[pos[1]]])))\n",
    "            \n",
    "            if(fitness(s,trainX, testX, trainy, testy) < fit[index_w[j]]):\n",
    "                fit[index_w[j]] = fitness(s,trainX, testX, trainy, testy)\n",
    "                pop[index_w[j]] = s.copy()\n",
    "                \n",
    "                \n",
    "        for j in range(len(index_b)):\n",
    "            random.seed(time.time())\n",
    "            if(random.random() < bp): \n",
    "                l, nl = [], []\n",
    "                for k in range(dimension):\n",
    "                    random.seed(time.time()+k*2)\n",
    "                    l.append(random.random())\n",
    "\n",
    "                for k in l:\n",
    "                    nl.append(1-k)\n",
    "\n",
    "                s = np.add(np.multiply(nl, pop[index_b[j]]), np.multiply(l, (np.subtract(bestpop, pop[index_b[j]]))))\n",
    "\n",
    "                if(fitness(s,trainX, testX, trainy, testy) < fit[index_b[j]]):\n",
    "                    fit[index_b[j]] = fitness(s,trainX, testX, trainy, testy)\n",
    "                    pop[index_b[j]] = s.copy()  \n",
    "                    \n",
    "        for j in range(pop_size):\n",
    "            for k in range(dimension):\n",
    "                random.seed(time.time())\n",
    "                if (sigmoid(pop[j][k]) > 0.5):\n",
    "                    pop[j][k] = 1\n",
    "                    \n",
    "                else:\n",
    "                    pop[j][k] = 0\n",
    "                \n",
    "                \n",
    "        fit = allfit(pop, trainX, testX, trainy, testy)\n",
    "        ind = np.argsort(fit)\n",
    "        index_b, index_w = [], []\n",
    "\n",
    "        for i in range(bsize):\n",
    "            index_b.append(ind[i])\n",
    "\n",
    "        for i in range(wsize):\n",
    "            index_w.append(ind[i])\n",
    "\n",
    "        bestpop = pop[ind[0]]\n",
    "        bestfit = fit[ind[0]]\n",
    "    \n",
    "    fit = allfit(pop, trainX, testX, trainy, testy)\n",
    "    ind = np.argsort(fit)\n",
    "    bestpop = pop[ind[0]]\n",
    "    bestfit = fit[ind[0]]\n",
    "    \n",
    "    gbest = bestpop[0]\n",
    "    for i in range(len(pop)):\n",
    "        if (fitness(pop[i], trainX, testX, trainy, testy) < fitness(gbest, trainX, testX, trainy, testy)):\n",
    "            gbest = deepcopy(pop[i])\n",
    "    \n",
    "    \n",
    "    testAcc = test_accuracy(bestpop, trainX, testX, trainy, testy)\n",
    "    featCnt = onecnt(bestpop)\n",
    "    #print(\"best agent: \", bestpop)\n",
    "    print(\"Test Accuracy: \", testAcc)\n",
    "    print(\"#Features: \", featCnt)\n",
    "            \n",
    "    return testAcc, featCnt, bestpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Acc:  0.9692307692307692\n",
      "Test Accuracy:  0.963076923076923\n",
      "#Features:  87.0\n",
      "Wall time: 3h 41min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Ta,fc,bp = NMRA('IITM128.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.048293</td>\n",
       "      <td>0.844509</td>\n",
       "      <td>1.892785</td>\n",
       "      <td>0.583005</td>\n",
       "      <td>0.167826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>Assamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>0.038213</td>\n",
       "      <td>0.577700</td>\n",
       "      <td>0.434400</td>\n",
       "      <td>0.332166</td>\n",
       "      <td>0.875804</td>\n",
       "      <td>0.374820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>Assamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.018744</td>\n",
       "      <td>0.107425</td>\n",
       "      <td>0.391567</td>\n",
       "      <td>0.058652</td>\n",
       "      <td>0.285366</td>\n",
       "      <td>0.906578</td>\n",
       "      <td>0.844500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>Assamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.035911</td>\n",
       "      <td>0.188132</td>\n",
       "      <td>0.699432</td>\n",
       "      <td>0.101537</td>\n",
       "      <td>0.304569</td>\n",
       "      <td>0.600820</td>\n",
       "      <td>1.438876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>Assamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>0.046121</td>\n",
       "      <td>0.050567</td>\n",
       "      <td>0.278968</td>\n",
       "      <td>1.727229</td>\n",
       "      <td>4.493108</td>\n",
       "      <td>6.378520</td>\n",
       "      <td>1.567698</td>\n",
       "      <td>1.897226</td>\n",
       "      <td>1.165529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>Assamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3245</td>\n",
       "      <td>0.039602</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>0.374282</td>\n",
       "      <td>8.345696</td>\n",
       "      <td>16.802679</td>\n",
       "      <td>16.385887</td>\n",
       "      <td>20.768005</td>\n",
       "      <td>6.230567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.003456</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>Telugu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3246</td>\n",
       "      <td>0.149945</td>\n",
       "      <td>0.070613</td>\n",
       "      <td>0.016239</td>\n",
       "      <td>0.052733</td>\n",
       "      <td>0.622141</td>\n",
       "      <td>11.021016</td>\n",
       "      <td>27.996975</td>\n",
       "      <td>15.667769</td>\n",
       "      <td>4.889742</td>\n",
       "      <td>2.230314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Telugu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3247</td>\n",
       "      <td>0.549227</td>\n",
       "      <td>0.358309</td>\n",
       "      <td>0.153024</td>\n",
       "      <td>0.153383</td>\n",
       "      <td>1.099469</td>\n",
       "      <td>9.914358</td>\n",
       "      <td>26.537991</td>\n",
       "      <td>5.615302</td>\n",
       "      <td>2.793209</td>\n",
       "      <td>2.330279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>0.008005</td>\n",
       "      <td>0.008298</td>\n",
       "      <td>0.006930</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>Telugu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3248</td>\n",
       "      <td>0.303473</td>\n",
       "      <td>0.193823</td>\n",
       "      <td>0.065367</td>\n",
       "      <td>0.135263</td>\n",
       "      <td>0.653783</td>\n",
       "      <td>7.623892</td>\n",
       "      <td>15.936741</td>\n",
       "      <td>15.815204</td>\n",
       "      <td>9.590445</td>\n",
       "      <td>4.106088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>0.009638</td>\n",
       "      <td>0.011048</td>\n",
       "      <td>0.007675</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>Telugu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3249</td>\n",
       "      <td>0.608493</td>\n",
       "      <td>0.231677</td>\n",
       "      <td>0.059475</td>\n",
       "      <td>0.077860</td>\n",
       "      <td>0.572939</td>\n",
       "      <td>10.690310</td>\n",
       "      <td>35.973400</td>\n",
       "      <td>20.046862</td>\n",
       "      <td>5.479385</td>\n",
       "      <td>1.560419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010415</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.011469</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Telugu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3250 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4          5          6  \\\n",
       "0     0.009862  0.011445  0.005392  0.001609  0.000516   0.048293   0.844509   \n",
       "1     0.000180  0.000934  0.000941  0.007122  0.038213   0.577700   0.434400   \n",
       "2     0.000322  0.001019  0.001124  0.018744  0.107425   0.391567   0.058652   \n",
       "3     0.000416  0.000962  0.002874  0.035911  0.188132   0.699432   0.101537   \n",
       "4     0.016845  0.046121  0.050567  0.278968  1.727229   4.493108   6.378520   \n",
       "...        ...       ...       ...       ...       ...        ...        ...   \n",
       "3245  0.039602  0.016221  0.010791  0.045113  0.374282   8.345696  16.802679   \n",
       "3246  0.149945  0.070613  0.016239  0.052733  0.622141  11.021016  27.996975   \n",
       "3247  0.549227  0.358309  0.153024  0.153383  1.099469   9.914358  26.537991   \n",
       "3248  0.303473  0.193823  0.065367  0.135263  0.653783   7.623892  15.936741   \n",
       "3249  0.608493  0.231677  0.059475  0.077860  0.572939  10.690310  35.973400   \n",
       "\n",
       "              7          8         9  ...        78        79        80  \\\n",
       "0      1.892785   0.583005  0.167826  ...  0.000035  0.000058  0.000229   \n",
       "1      0.332166   0.875804  0.374820  ...  0.000139  0.000288  0.000561   \n",
       "2      0.285366   0.906578  0.844500  ...  0.000059  0.000116  0.000214   \n",
       "3      0.304569   0.600820  1.438876  ...  0.000014  0.000081  0.000329   \n",
       "4      1.567698   1.897226  1.165529  ...  0.000486  0.000693  0.000706   \n",
       "...         ...        ...       ...  ...       ...       ...       ...   \n",
       "3245  16.385887  20.768005  6.230567  ...  0.002938  0.003456  0.002945   \n",
       "3246  15.667769   4.889742  2.230314  ...  0.005599  0.007398  0.006048   \n",
       "3247   5.615302   2.793209  2.330279  ...  0.007095  0.008343  0.008005   \n",
       "3248  15.815204   9.590445  4.106088  ...  0.007039  0.008723  0.009638   \n",
       "3249  20.046862   5.479385  1.560419  ...  0.010415  0.018541  0.011469   \n",
       "\n",
       "            81        82        83        84        85        86     label  \n",
       "0     0.000238  0.000488  0.000551  0.000330  0.000254  0.000186  Assamese  \n",
       "1     0.000966  0.000420  0.000471  0.000441  0.000196  0.000147  Assamese  \n",
       "2     0.000318  0.000150  0.000183  0.000190  0.000131  0.000048  Assamese  \n",
       "3     0.000366  0.000219  0.000151  0.000180  0.000242  0.000305  Assamese  \n",
       "4     0.000615  0.000264  0.000245  0.000159  0.000139  0.000419  Assamese  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3245  0.002165  0.001306  0.000754  0.000368  0.000104  0.000010    Telugu  \n",
       "3246  0.005397  0.003318  0.002020  0.000938  0.000227  0.000011    Telugu  \n",
       "3247  0.008298  0.006930  0.005167  0.001888  0.000514  0.000028    Telugu  \n",
       "3248  0.011048  0.007675  0.005179  0.002411  0.000744  0.000040    Telugu  \n",
       "3249  0.006631  0.003809  0.002043  0.001857  0.000371  0.000011    Telugu  \n",
       "\n",
       "[3250 rows x 88 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List = []\n",
    "\n",
    "for i in range(0,len(bp)):\n",
    "    if bp[i] == 1:\n",
    "        List.append(i)\n",
    "        \n",
    "df_train = pd.read_csv(\"IITM128.csv\")\n",
    "y = (df_train['label'])\n",
    "df_train1 = df_train[df_train.columns[List]]\n",
    "l = []\n",
    "for i in range(0,len(List)):\n",
    "    l.append(i)\n",
    "    \n",
    "df_train1.columns = l\n",
    "\n",
    "df_train1['label'] = y\n",
    "df_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1.to_csv('IITM_Spec128_NMRAHS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
